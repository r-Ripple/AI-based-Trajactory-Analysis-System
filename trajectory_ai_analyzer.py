#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è½¨è¿¹æ•°æ®AIåˆ†æå™¨ - OpenAI SDKç‰ˆæœ¬
ä½¿ç”¨ openai.OpenAI() + base_url="https://api.poe.com/v1" è°ƒç”¨Poe API
"""
import time
import json
import os
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from datetime import datetime
import pandas as pd
import numpy as np

try:
    import openai
except ImportError:
    print("è¯·å®‰è£… openai: pip install openai")
    exit(1)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class AnalysisConfig:
    """åˆ†æé…ç½®"""
    api_key: str
    model_name: str = "GPT-4o"
    max_tokens: int = 4000
    temperature: float = 0.7
    analysis_types: List[str] = None
    # æ–°å¢è¾“å‡ºæ§åˆ¶é…ç½®
    output_level: str = "summary"  # "summary", "standard", "detailed"
    max_preview_length: int = 500
    save_detailed_separately: bool = True
    generate_markdown_report: bool = True

    def __post_init__(self):
        if self.analysis_types is None:
            self.analysis_types = [
                "temporal_comparative",  # å¿…é€‰1ï¼šæ—¶é—´å¯¹æ¯”åˆ†æ
                "spatial_differential",  # å¿…é€‰2ï¼šç©ºé—´å·®åˆ†åˆ†æ
                "spatiotemporal_transitions",  # å¿…é€‰3ï¼šæ—¶ç©ºè½¬åœºä¸é“¾æ¡
                "cross_feature_insights",  # å¿…é€‰4ï¼šè·¨ç»´å…³è”åˆ†æ
                "anomaly_explanatory",  # å¯é€‰ï¼šè§£é‡Šæ€§å¼‚å¸¸æ£€æµ‹
                "meta_synthesis"
            ]

class TrajectoryAIAnalyzer:
    """è½¨è¿¹æ•°æ®AIåˆ†æå™¨ - ä½¿ç”¨OpenAI SDKé€šè¿‡Poe"""
    
    def __init__(self, config: AnalysisConfig):
        self.config = config
        
        # åˆ›å»ºOpenAIå®¢æˆ·ç«¯ï¼ŒæŒ‡å‘Poeçš„base_url
        self.client = openai.OpenAI(
            api_key=config.api_key,
            base_url="https://api.poe.com/v1"
        )
        
        # åŠ è½½promptæ¨¡æ¿
        self.prompt_templates = self._load_prompt_templates()

    def _run_meta_synthesis(self, user_id: str, previous_results: Dict[str, Any],
                            user_data: Dict, geocoded_data: Optional[List[Dict]]) -> Dict[str, Any]:
        """
        æ‰§è¡Œç»¼åˆæ´å¯Ÿæ±‡æ€»åˆ†æ
        """
        logger.info(f"å¼€å§‹ç»¼åˆæ´å¯Ÿæ±‡æ€»...")

        # æ·»åŠ ç»“æœæ£€æŸ¥
        if not previous_results or all("error" in r for r in previous_results.values()):
            logger.error("æ²¡æœ‰å¯ç”¨çš„å‰ç½®åˆ†æç»“æœ")
            return {
                "error": "æ— æ³•è¿›è¡Œç»¼åˆåˆ†æï¼šç¼ºå°‘å‰ç½®åˆ†æç»“æœ",
                "analysis_type": "meta_synthesis",
                "timestamp": datetime.now().isoformat()
            }

        # å‡†å¤‡æ‰€æœ‰åˆ†æç»“æœçš„æ‘˜è¦
        all_results_summary = self._prepare_all_results_summary(previous_results)

        # æ£€æŸ¥æ‘˜è¦å†…å®¹
        if not all_results_summary or len(all_results_summary) < 100:
            logger.warning(f"å‰ç½®åˆ†æç»“æœæ‘˜è¦è¿‡çŸ­: {len(all_results_summary)} å­—ç¬¦")

        # æ„å»ºprompt
        prompt_data = {
            "all_analysis_results": all_results_summary
        }

        prompt = self._build_prompt("meta_synthesis", prompt_data)

        # è°ƒç”¨AI API
        start_time = time.time()
        response = self._call_ai_api11(prompt)
        end_time = time.time()

        # è§£æå“åº”
        parsed_result = self._parse_ai_response(response, "meta_synthesis")
        parsed_result["processing_time"] = end_time - start_time
        parsed_result["based_on_analyses"] = list(previous_results.keys())

        return parsed_result

    def _prepare_all_results_summary(self, previous_results: Dict[str, Any]) -> str:
        """
        å‡†å¤‡æ‰€æœ‰å·²å®Œæˆåˆ†æçš„ç»“æœæ‘˜è¦
        """
        summary_parts = []

        for analysis_type, result in previous_results.items():
            if "error" in result:
                continue

            summary_parts.append(f"## {analysis_type.upper()}\n")

            # ä¿®æ­£ï¼šä»æ­£ç¡®çš„ä½ç½®æå–content
            # ä¼˜å…ˆä» structured_result.details è·å–ï¼Œå…¶æ¬¡ä» raw_responseï¼Œæœ€åä» content
            content = ""
            if "structured_result" in result:
                content = result["structured_result"].get("details", "")
            if not content and "raw_response" in result:
                content = result.get("raw_response", "")
            if not content:
                content = result.get("content", "")

            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰å†…å®¹ï¼Œå°è¯•æ„å»ºä¸€ä¸ªåŸºç¡€æ‘˜è¦
            if not content:
                structured = result.get("structured_result", {})
                content_parts = []
                if structured.get("summary"):
                    content_parts.append(f"æ‘˜è¦: {structured['summary']}")
                if structured.get("highlights"):
                    content_parts.append("è¦ç‚¹: " + "; ".join(structured['highlights']))
                if structured.get("metrics"):
                    content_parts.append("æŒ‡æ ‡: " + str(structured['metrics']))
                content = "\n".join(content_parts) if content_parts else "æ— è¯¦ç»†å†…å®¹"

            # æ ¹æ®è¾“å‡ºçº§åˆ«å†³å®šåŒ…å«å¤šå°‘å†…å®¹
            if self.config.output_level == "summary":
                summary_parts.append(content[:800] + "...\n" if len(content) > 800 else content + "\n")
            elif self.config.output_level == "standard":
                summary_parts.append(content[:1500] + "...\n" if len(content) > 1500 else content + "\n")
            else:
                summary_parts.append(content + "\n")

            summary_parts.append("\n" + "=" * 80 + "\n\n")

        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        if not summary_parts:
            logger.warning("è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯ç”¨çš„åˆ†æç»“æœå†…å®¹")
            return "æœªæ‰¾åˆ°å‰ç½®åˆ†æç»“æœçš„è¯¦ç»†å†…å®¹"

        result_text = "".join(summary_parts)
        logger.info(f"å‡†å¤‡çš„ç»¼åˆåˆ†æè¾“å…¥é•¿åº¦: {len(result_text)} å­—ç¬¦")

        return result_text
    
    def _load_prompt_templates(self) -> Dict[str, str]:
        """åŠ è½½åˆ†ææç¤ºæ¨¡æ¿"""
        return {
            "system": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„äººç±»ç§»åŠ¨è¡Œä¸ºå’Œè½¨è¿¹æ•°æ®åˆ†æä¸“å®¶ã€‚ä¸“æ³¨å‘ç°"äººçœ¼éš¾å¯Ÿè§‰ä½†æ•°æ®æ­ç¤º"çš„é«˜é˜¶æ¨¡å¼ã€‚

æ ¸å¿ƒè¦æ±‚ï¼š
1. ä»"åˆ†å¸ƒæè¿°"å‡çº§ä¸º"å¯¹æ¯”æ´å¯Ÿ"ï¼šå¿…é¡»äº§å‡ºå¸¦å·®å€¼çš„å‘ç°
2. é¿å…æ³›åŒ–åœ°åæè¿°ï¼Œèšç„¦ç©ºé—´-åŠŸèƒ½è€¦åˆå’Œçƒ­ç‚¹åç§»
3. ç”¨æ´»åŠ¨é“¾ï¼ˆPOI+æ—¶æ®µ+åœç•™æ—¶é•¿ï¼‰ä¸è½¬åœºè§„åˆ™è¡¨è¾¾æ—¶ç©ºå…³ç³»
4. äº§å‡ºå¯æ£€éªŒçš„å…³è”é™ˆè¿°ï¼Œè€Œéå¤è¿°æ€§æè¿°
5. å¼‚å¸¸å¿…é¡»å…·å¤‡è§£é‡Šä»·å€¼ï¼Œå½±å“æ•´ä½“ç»“è®º

è¾“å‡ºæ ¼å¼ï¼š
- summary: æœ€é‡è¦å‘ç°ï¼ˆ50å­—å†…ï¼‰
- insights: 3-5æ¡éç›´è§‚æ´å¯Ÿï¼ˆå«pattern/evidence/interpretation/novelty_scoreï¼‰
- metrics: å…³é”®æŒ‡æ ‡å¯¹æ¯”
- details: æ·±åº¦åˆ†æ

ä¸¥æ ¼é¿å…ï¼šæ³›è¿°æ€§ç»“è®ºã€æ˜¾è€Œæ˜“è§çš„æ¨¡å¼ã€æ— å¯¹æ¯”çš„åˆ†å¸ƒæè¿°
""",




            "spatial_differential": """## ä»»åŠ¡ï¼šç©ºé—´æ¨¡å¼å·®åˆ†åˆ†æï¼ˆå‡çº§ç‰ˆï¼‰

### è¾“å…¥æ•°æ®
{spatial_data}
{geocoded_context}

**æ ¸å¿ƒä»»åŠ¡ï¼šçƒ­ç‚¹åç§»+æ–¹å‘æ€§+ç©ºé—´å·®åˆ†ï¼Œé¿å…æ³›åŒ–åœ°å**

å¿…é¡»è¾“å‡ºï¼š
1. **çƒ­ç‚¹é‡å¿ƒåç§»åˆ†æ**ï¼š
   - å‘¨æœŸæ€§é‡å¿ƒåç§»ï¼šå·¥ä½œæ—¥vså‘¨æœ«çš„é‡å¿ƒåæ ‡å·®ï¼ˆç±³ï¼‰
   - åç§»å¹…åº¦ï¼šé‡å¿ƒç§»åŠ¨è·ç¦»çš„ç»Ÿè®¡åˆ†å¸ƒ
   - åç§»æ–¹å‘ï¼šä¸»å¯¼åç§»å‘é‡ï¼ˆè§’åº¦+è·ç¦»ï¼‰

2. **èµ°å»Šæ–¹å‘æ€§åˆ†æ**ï¼š
   - æ–¹å‘ç«ç‘°ï¼š8æ–¹å‘å‡ºè¡Œæ¯”ä¾‹å·®å¼‚
   - å¾€è¿”ä¸å¯¹ç§°æ€§ï¼šå»ç¨‹vså›ç¨‹çš„è·¯å¾„å·®å¼‚åº¦
   - æ–¹å‘åå¥½å¼ºåº¦ï¼šè§’åº¦é›†ä¸­åº¦æŒ‡æ ‡

3. **æ¢ç´¢åº¦ä¸é‡è®¿ç‡å·®åˆ†**ï¼š
   - æ–°åœ°ç‚¹vsé‡è®¿åœ°ç‚¹çš„æ—¶ç©ºåˆ†å¸ƒå·®å¼‚
   - æ¢ç´¢åŠå¾„æ‰©å¼ é€Ÿç‡ï¼šæŒ‰æ—¶é—´åºåˆ—çš„åŠå¾„å¢é•¿ç‡
   - é‡è®¿å¼ºåº¦æ¢¯åº¦ï¼šä¸åŒè·ç¦»åœˆçš„é‡è®¿é¢‘æ¬¡é€’å‡ç‡

4. **ç©ºé—´-åŠŸèƒ½è€¦åˆåˆ†æ**ï¼š
   - POIåŠŸèƒ½åŒºçš„åœç•™æ—¶é•¿åˆ†å¸ƒå·®å¼‚
   - åŠŸèƒ½è½¬æ¢çš„ç©ºé—´è·ç¦»æ¨¡å¼
   - åŠŸèƒ½é“¾æ¡ï¼šAåŠŸèƒ½â†’BåŠŸèƒ½çš„ç©ºé—´è·³è·ƒè·ç¦»åˆ†å¸ƒ

è¾“å‡ºè¦æ±‚ï¼šç”¨å‡¸åŒ…/æ ¸å¯†åº¦å·®åˆ†é‡åŒ–ï¼Œæä¾›åæ ‡å·®å€¼å’Œé¢ç§¯å˜åŒ–
""",

            "spatiotemporal_transitions": """## ä»»åŠ¡ï¼šæ—¶ç©ºè½¬åœºä¸æ´»åŠ¨é“¾åˆ†æï¼ˆé‡æ„ç‰ˆï¼‰

### è¾“å…¥æ•°æ®
{comprehensive_data}
{stays_summary}
{trips_summary}

**æ ¸å¿ƒä»»åŠ¡ï¼šé‡æ„è¡Œä¸ºæ¨¡å¼åˆ†æï¼Œèšç„¦æ´»åŠ¨é“¾ä¸è½¬åœºè§„åˆ™**

å¿…é¡»è¾“å‡ºï¼š
1. **æ´»åŠ¨é“¾ç»“æ„è¯†åˆ«**ï¼š
   - å…¸å‹æ´»åŠ¨é“¾ï¼šPOIç±»å‹+æ—¶æ®µ+åœç•™æ—¶é•¿çš„ç»„åˆæ¨¡å¼
   - é“¾æ¡å®Œæ•´åº¦ï¼šèµ·-æ‰¿-è½¬-åˆçš„å®Œæ•´æ€§è¯„åˆ†
   - é“¾æ¡å˜å¼‚ï¼šæ ‡å‡†é“¾vså®é™…é“¾çš„åå·®åˆ†æ

2. **è½¬åœºè§„åˆ™é‡åŒ–**ï¼š
   - æ¡ä»¶æ¦‚ç‡çŸ©é˜µï¼šè‹¥åœ¨A(POI+æ—¶æ®µ)ï¼Œåˆ™ä¸‹ä¸€æ­¥åˆ°Bçš„æ¦‚ç‡
   - æ—¶é—´å·®ä¾èµ–ï¼šè½¬åœºæ—¶é—´é—´éš”å¯¹ç›®æ ‡é€‰æ‹©çš„å½±å“è§„å¾‹
   - è·ç¦»è¡°å‡å‡½æ•°ï¼šè½¬åœºè·ç¦»å¯¹é€‰æ‹©æ¦‚ç‡çš„è¡°å‡æ¨¡å‹

3. **æ—¥å†…ç­–ç•¥æ¨¡å¼**ï¼š
   - æ—©-ä¸­-æ™šä¸‰æ—¶æ®µçš„ç­–ç•¥å·®å¼‚
   - æ—¶æ®µå†…çš„å¾®è°ƒç­–ç•¥ï¼šåŒç±»æ´»åŠ¨çš„æ—¶ç©ºå¾®è°ƒæ¨¡å¼
   - ç­–ç•¥åº”æ€¥æ€§ï¼šè®¡åˆ’å¤–è½¬åœºçš„åº”å¯¹æ¨¡å¼

4. **å› æœåç§»æ•ˆåº”**ï¼š
   - å‰æ•ˆåº”ï¼šå‰ä¸€æ´»åŠ¨å¯¹å½“å‰é€‰æ‹©çš„å½±å“å¼ºåº¦å’Œè¡°å‡
   - åæ•ˆåº”ï¼šå¯¹æœªæ¥æ´»åŠ¨çš„é¢„æœŸå¦‚ä½•å½±å“å½“å‰å†³ç­–
   - æ—¶æ»åˆ†æï¼šæ•ˆåº”ä¼ é€’çš„æ—¶é—´å»¶è¿Ÿåˆ†å¸ƒ

è¾“å‡ºè¦æ±‚ï¼šæä¾›å…·ä½“çš„è½¬åœºæ¦‚ç‡æ•°å€¼ã€æ—¶é—´å·®ç»Ÿè®¡ã€å› æœå¼ºåº¦è¯„åˆ†
""",

            "cross_feature_insights": """## ä»»åŠ¡ï¼šè·¨ç»´å…³è”æ´å¯Ÿï¼ˆé‡æ„ç‰ˆï¼‰

            ### è¾“å…¥æ•°æ®
            {comprehensive_data}
            {pattern_data}

            **æ ¸å¿ƒä»»åŠ¡ï¼šä»æ³›è¿°è½¬ä¸ºå¯æ£€éªŒçš„å…³è”é™ˆè¿°ï¼Œä½“ç°AIæ´å¯Ÿä»·å€¼**

            å¿…é¡»è¾“å‡ºï¼š
            1. **åŠå¾„-æ—¶é—´ç†µ-å¤œé—´å æ¯”ä¸‰å…ƒå…³è”**ï¼š
               - å…³è”å¼ºåº¦ï¼šæ´»åŠ¨åŠå¾„â†‘ â‡” æ—¶é—´ç†µâ†‘ â‡” å¤œé—´å æ¯”â†‘çš„ç›¸å…³ç³»æ•°
               - é˜ˆå€¼è¯†åˆ«ï¼šå…³è”å‘ç”Ÿçªå˜çš„ä¸´ç•Œå€¼ç‚¹
               - å¼‚å¸¸ä¸ªä½“ï¼šåç¦»ä¸‰å…ƒå…³è”çš„å¼‚å¸¸æ¨¡å¼åŠå…¶è§£é‡Š

            2. **è·¯å¾„é‡å¤ç‡ä¸æ—¶é—´ç­–ç•¥å…³è”**ï¼š
               - é‡å¤ç‡-å‡ºå‘æ—¶å·®å…³ç³»ï¼šè·¯å¾„ç†Ÿæ‚‰åº¦å¦‚ä½•å½±å“å‡ºå‘æ—¶é—´ç²¾å‡†åº¦
               - æ¢ç´¢æˆæœ¬-æ—¶é—´å†—ä½™å…³ç³»ï¼šæ–°è·¯å¾„æ¢ç´¢çš„æ—¶é—´æˆæœ¬é‡åŒ–
               - æ•ˆç‡-çµæ´»æ€§æƒè¡¡ï¼šé‡å¤vsæ¢ç´¢çš„æ—¶ç©ºæ•ˆç‡å¯¹æ¯”

            3. **åœç•™æ—¶é•¿-POIå¯†åº¦-å›è®¿é—´éš”å…³è”**ï¼š
               - ä¸‰å˜é‡å…³è”æ¨¡å‹ï¼šåœç•™æ—¶é•¿å¦‚ä½•å—POIå¯†åº¦å’Œå›è®¿é—´éš”å½±å“
               - é¥±å’Œæ•ˆåº”è¯†åˆ«ï¼šPOIå¯†åº¦å¯¹åœç•™æ—¶é•¿å½±å“çš„é¥±å’Œç‚¹
               - è®°å¿†è¡°å‡æ•ˆåº”ï¼šå›è®¿é—´éš”å¯¹åœç•™æ—¶é•¿çš„å½±å“è§„å¾‹

            4. **æ˜¼å¤œåˆ‡æ¢-è·ç¦»é€‰æ‹©-åŠŸèƒ½åå¥½å…³è”**ï¼š
               - æ—¶æ®µ-è·ç¦»é€‰æ‹©æ¨¡å¼ï¼šæ˜¼å¤œæ—¶æ®µå¦‚ä½•å½±å“å‡ºè¡Œè·ç¦»é€‰æ‹©
               - åŠŸèƒ½-æ—¶æ®µè€¦åˆå¼ºåº¦ï¼šä¸åŒPOIåŠŸèƒ½çš„æ—¶æ®µåå¥½å¼ºåº¦
               - åˆ‡æ¢æˆæœ¬ï¼šæ˜¼å¤œåŠŸèƒ½åˆ‡æ¢çš„ç©ºé—´æˆæœ¬åˆ†æ

            è¾“å‡ºè¦æ±‚ï¼šæ¯é¡¹å…³è”å¿…é¡»æä¾›ç›¸å…³ç³»æ•°ã€ç»Ÿè®¡æ˜¾è‘—æ€§ã€å…·ä½“é˜ˆå€¼æ•°æ®
            ""","temporal_comparative": """## ä»»åŠ¡ï¼šæ—¶é—´æ¨¡å¼å¯¹æ¯”åˆ†æï¼ˆå‡çº§ç‰ˆï¼‰

### è¾“å…¥æ•°æ®
{temporal_data}

**æ ¸å¿ƒä»»åŠ¡ï¼šä»åˆ†å¸ƒå‡çº§ä¸ºå¯¹æ¯”ï¼Œäº§å‡ºå¸¦å·®å€¼çš„æ´å¯Ÿ**

å¿…é¡»è¾“å‡ºï¼š
1. **å·¥ä½œæ—¥vså‘¨æœ«å¯¹æ¯”**ï¼š
   - å‡ºè¡Œæ¬¡æ•°å·®å€¼ï¼šå…·ä½“æ•°å­—+ç™¾åˆ†æ¯”å˜åŒ–
   - æ´»è·ƒæ—¶æ®µåç§»ï¼šå³°å€¼æ—¶é—´å·®å¼‚ï¼ˆå°æ—¶ï¼‰
   - èŠ‚å¾‹ç¨³å®šæ€§å·®å¼‚ï¼šç†µå€¼å˜åŒ–é‡

2. **æ­£å¸¸æ—¥vså¼‚å¸¸æ—¥è¯†åˆ«**ï¼š
   - å¼‚å¸¸æ—¥å®šä¹‰æ ‡å‡†ï¼ˆåŸºäºå‡ºè¡Œé‡/æ—¶é•¿/æ¨¡å¼çªå˜ï¼‰
   - å¼‚å¸¸æ—¥è¡Œä¸ºç‰¹å¾é‡åŒ–å·®å¼‚
   - å¼‚å¸¸æ—¥å¯¹æ•´ä½“æ¨¡å¼çš„å½±å“ç¨‹åº¦

3. **Top/Bottomå‡ºè¡Œæ—¥åˆ†æ**ï¼š
   - æœ€é«˜/æœ€ä½å‡ºè¡Œæ—¥çš„è¡Œä¸ºå·®å¼‚
   - è§¦å‘å› å­æ¨æ–­ï¼ˆå¤©æ°”/èŠ‚å‡æ—¥/ç‰¹æ®Šäº‹ä»¶ï¼‰
   - å› æœåç§»æ•ˆåº”ï¼šå‰/åæ—¥è¡Œä¸ºè¡¥å¿æœºåˆ¶

4. **èŠ‚å¾‹è½¬åœºé€»è¾‘**ï¼š
   - å‡ºè¡Œå¯åŠ¨çš„æ—¶é—´è§¦å‘è§„å¾‹
   - åœç•™ç»“æŸçš„æ—¶é—´å†³ç­–æ¨¡å¼
   - æ˜¼å¤œåˆ‡æ¢çš„è¡Œä¸ºç­–ç•¥å˜åŒ–

è¾“å‡ºè¦æ±‚ï¼šæ¯é¡¹å¿…é¡»åŒ…å«å…·ä½“æ•°å€¼å·®å¼‚ï¼Œé¿å…"æ›´æ´»è·ƒ"ç­‰æ¨¡ç³Šæè¿°
""",
            "anomaly_explanatory": """## ä»»åŠ¡ï¼šè§£é‡Šæ€§å¼‚å¸¸æ£€æµ‹ï¼ˆç²¾ç®€ç‰ˆï¼‰

        ### è¾“å…¥æ•°æ®
        {pattern_data}

        **æ ¸å¿ƒä»»åŠ¡ï¼šä»…è¯†åˆ«å½±å“æ•´ä½“ç»“è®ºè§£é‡Šçš„å…³é”®å¼‚å¸¸ï¼Œé¿å…ç½—åˆ—æ‰€æœ‰å¼‚å¸¸**

        é™åˆ¶è¾“å‡º1-2æ¡è§£é‡Šæ€§å¼‚å¸¸ï¼Œè¦æ±‚ï¼š
        1. **å¼‚å¸¸è¯†åˆ«æ ‡å‡†**ï¼š
           - åç¦»æ­£å¸¸æ¨¡å¼çš„ç»Ÿè®¡é˜ˆå€¼ï¼ˆ3Ïƒæˆ–åˆ†ä½æ•°æ ‡å‡†ï¼‰
           - å¼‚å¸¸å¯¹æ•´ä½“æ¨¡å¼è§£é‡Šçš„å½±å“æƒé‡è¯„ä¼°

        2. **è§£é‡Šæ€§ä»·å€¼è¯„ä¼°**ï¼š
           - å‰”é™¤å¼‚å¸¸å‰åçš„æ ¸å¿ƒæŒ‡æ ‡å·®å¼‚ï¼ˆå…·ä½“æ•°å€¼å˜åŒ–ï¼‰
           - å¼‚å¸¸æ˜¯å¦æ”¹å˜å¯¹ç”¨æˆ·è¡Œä¸ºçš„æ•´ä½“åˆ¤æ–­
           - å¼‚å¸¸èƒŒåçš„å¯èƒ½è§£é‡Šæœºåˆ¶

        è¾“å‡ºè¦æ±‚ï¼š
        - åªæŠ¥å‘Šä¼šæ”¹å˜æ•´ä½“ç»“è®ºçš„å¼‚å¸¸
        - æä¾›å‰”é™¤å‰åçš„å…³é”®æŒ‡æ ‡å¯¹æ¯”
        - è¯´æ˜å¼‚å¸¸çš„è§£é‡Šä»·å€¼å’Œå¤„ç†å»ºè®®
        - é¿å…æŠ€æœ¯æ€§å¼‚å¸¸ç½—åˆ—ï¼Œèšç„¦è¡Œä¸ºè§£é‡Šæ„ä¹‰
        """
            ,

            "meta_synthesis": """## ä»»åŠ¡ï¼šç»¼åˆæ´å¯Ÿæ±‡æ€»ä¸å…ƒåˆ†æ

ä½ å·²ç»å®Œæˆäº†ç”¨æˆ·çš„å¤šç»´è½¨è¿¹åˆ†æï¼Œç°åœ¨éœ€è¦æ•´åˆå…¨éƒ¨ç»“æœï¼Œæç‚¼æ›´é«˜å±‚æ¬¡çš„è¡Œä¸ºè§„å¾‹ä¸æœ¬è´¨ç‰¹å¾ã€‚ä½†æ˜¯éœ€è¦ç‰¹åˆ«æ³¨æ„çš„æ˜¯ï¼Œä½ è¾“å‡ºçš„å†…å®¹æ˜¯ç»™æˆ‘çš„è€æ¿æ±‡æŠ¥çš„ï¼Œä»–å¯¹äºä¸“ä¸šçŸ¥è¯†ä¸€çªä¸é€šï¼Œéœ€è¦ä½ æŠŠè¡Œä¸ºä¸ç‰¹å¾ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è¿›è¡Œæè¿°



### å·²å®Œæˆçš„åˆ†æç»“æœ
{all_analysis_results}



## æ ¸å¿ƒä»»åŠ¡ï¼šæç‚¼è·¨ç»´åº¦çš„æ·±å±‚æ¨¡å¼ä¸è¡Œä¸ºæœ¬è´¨,ç”¨æ›´åŠ é€šä¿—æ˜“æ‡‚çš„è¯­è¨€å°†ä¸‹é¢çš„è¦æ±‚å®ç°ã€‚



### 1. è·¨ç»´åº¦æ¨¡å¼æ•´åˆ (Cross-Dimensional Pattern Integration)
**ç›®æ ‡**ï¼šåœ¨ä¸åŒåˆ†æç»´åº¦ä¹‹é—´æ‰¾å‡ºå‘¼åº”ã€çŸ›ç›¾æˆ–äº’è¡¥çš„è§„å¾‹ï¼Œè®©ç»“è®ºä¹‹é—´å½¢æˆé€»è¾‘é—­ç¯ã€‚

åˆ†ææ–¹å‘ï¼š
- **æ—¶é—´â€“ç©ºé—´è€¦åˆ**ï¼šæ—¶é—´è§„å¾‹å’Œç©ºé—´åˆ†å¸ƒå¦‚ä½•ç›¸äº’è§£é‡Šï¼Ÿ  
  - ä¾‹ï¼šå·¥ä½œæ—¥å³°å€¼æ—¶é—´æ˜¯å¦ä¸ç©ºé—´æ´»åŠ¨é‡å¿ƒå˜åŒ–å¯¹åº”ï¼Ÿ  
  - å‘¨æœ«å‡ºè¡Œæ—¶é—´å˜åŒ–æ˜¯å¦ä¼´éšæ¢ç´¢åŠå¾„æ‰©å¤§ï¼Ÿ
- **è½¬åœºâ€“å…³è”è€¦åˆ**ï¼šæ—¶ç©ºè½¬æ¢è§„å¾‹æ˜¯å¦éªŒè¯äº†è·¨ç»´åº¦çš„å‡è®¾ï¼Ÿ  
  - ä¾‹ï¼šåœç•™æ—¶é•¿ä¸POIå¯†åº¦çš„å…³ç³»æ˜¯å¦èƒ½åœ¨çœŸå®è¿ç§»åŠ¨çº¿ä¸­ä½“ç°ï¼Ÿ
- **çŸ›ç›¾ä¸å¼ åŠ›è¯†åˆ«**ï¼šä¸åŒç»´åº¦é—´æœ‰æ— è¡¨é¢å†²çªæˆ–åå¸¸ç»„åˆï¼Ÿ  
  - ä¾‹ï¼šç©ºé—´é›†ä¸­å´æ—¶é—´åˆ†æ•£ï¼Œè¯´æ˜åŒåœ°ä½†ä½œæ¯å¤šæ ·ç­‰ã€‚




### 2. è¡Œä¸ºæœ¬è´¨å½’å›  (Behavioral Essence Attribution)

**ç›®æ ‡**ï¼šæ·±å…¥æ•°æ®èƒŒåï¼Œè§£é‡Šâ€œè¿™ä¸ªäººä¸ºä»€ä¹ˆè¿™æ ·ç§»åŠ¨â€ã€‚  

éœ€å®Œæˆä»¥ä¸‹åˆ†æï¼š

- **ä¸»å¯¼çº¦æŸè¯†åˆ«**  
  åˆ¤æ–­æ—¶é—´çº¦æŸã€ç©ºé—´çº¦æŸã€åŠŸèƒ½çº¦æŸä¸­å“ªä¸ªæœ€èƒ½è§£é‡Šå…¶è¡Œä¸ºè§„å¾‹ã€‚  
  - ç»™å‡ºè¯„åˆ†ï¼ˆ1â€“10åˆ†ï¼‰ï¼Œå¹¶ç®€è¿°ç†ç”±ï¼ˆåŸºäºå“ªäº›æ•°æ®ï¼‰ã€‚
  



### 3. æ ¸å¿ƒè¡Œä¸ºç”»åƒ (Core Behavioral Profile)

**ç›®æ ‡**ï¼šåœ¨ 100 å­—ä»¥å†…ï¼Œç”¨è‡ªç„¶å¹³å®çš„è¯­è¨€åˆ»ç”»è¯¥ç”¨æˆ·çš„æ ¸å¿ƒç§»åŠ¨ç‰¹å¾ã€‚  
è¦æ±‚ï¼š
- åŸºäºä¸Šæ–‡ç»¼åˆç»“è®º  
- ä¸ä½¿ç”¨æ¨¡æ¿åŒ–å¥å¼  
- è¯­è¨€æµç•…




### è¾“å‡ºé£æ ¼ä¸è¯­è¨€è¦æ±‚

- é€»è¾‘æ¸…æ™°ä½†è¯­è¨€é€šé¡ºã€è‡ªç„¶ã€äººæ€§åŒ–ï¼Œå°½é‡ä¸åŒ…å«è¿‡äºå­¦æœ¯çš„è¯­è¨€ 
- é¿å…åƒµç¡¬å¥å¼ï¼ˆå¦‚â€œä½“ç°å‡ºâ€¦â€¦ç‰¹å¾â€ï¼‰ï¼Œä½¿ç”¨æ›´æµç•…è¡¨è¾¾  
- æœ€åé™„ä¸Šä¸€ä¸ª **â€œè½»é‡ç‰ˆç”¨æˆ·ç”»åƒæ‘˜è¦â€**ï¼Œå†™æˆ 2â€“3 å¥å°ç»“


"""
        }


    def _call_ai_api(self, prompt: str) -> str:
        """
        ä½¿ç”¨OpenAI SDKè°ƒç”¨Poe API
        """
        try:
            # æ ¹æ®è¾“å‡ºçº§åˆ«è°ƒæ•´prompt
            if self.config.output_level == "summary":
                prompt = f"{prompt}\n\nè¯·æä¾›ç®€æ´çš„åˆ†æï¼Œé‡ç‚¹çªå‡ºå…³é”®å‘ç°ã€‚æ¯ä¸ªéƒ¨åˆ†æ§åˆ¶åœ¨200å­—ä»¥å†…ã€‚"
            elif self.config.output_level == "standard":
                prompt = f"{prompt}\n\nè¯·æä¾›æ ‡å‡†åˆ†æï¼Œå¹³è¡¡è¯¦ç»†åº¦å’Œå¯è¯»æ€§ã€‚"

            # ä½¿ç”¨OpenAI SDKçš„chat completions API
            response = self.client.chat.completions.create(
                model=self.config.model_name,
                messages=[
                    {"role": "system", "content": self.prompt_templates["system"]},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=self.config.max_tokens if self.config.output_level == "detailed" else min(2000,
                                                                                                     self.config.max_tokens),
                temperature=self.config.temperature
            )

            # æå–å“åº”æ–‡æœ¬
            return response.choices[0].message.content

        except Exception as e:
            logger.error(f"AI APIè°ƒç”¨å¤±è´¥: {str(e)}")
            return f"AIåˆ†æå¤±è´¥: {str(e)}"
    def _call_ai_api11(self, prompt: str) -> str:
        """
        ä½¿ç”¨OpenAI SDKè°ƒç”¨Poe API
        """
        try:
            # æ ¹æ®è¾“å‡ºçº§åˆ«è°ƒæ•´prompt
            if self.config.output_level == "summary":
                prompt = f"{prompt}\n\nè¯·æä¾›ç®€æ´çš„åˆ†æï¼Œé‡ç‚¹çªå‡ºå…³é”®å‘ç°ã€‚æ¯ä¸ªéƒ¨åˆ†æ§åˆ¶åœ¨200å­—ä»¥å†…ã€‚"
            elif self.config.output_level == "standard":
                prompt = f"{prompt}\n\nè¯·æä¾›æ ‡å‡†åˆ†æï¼Œå¹³è¡¡è¯¦ç»†åº¦å’Œå¯è¯»æ€§ã€‚"

            # ä½¿ç”¨OpenAI SDKçš„chat completions API
            response = self.client.chat.completions.create(
                model=self.config.model_name,
                messages=[
                    {"role": "system", "content": self.prompt_templates["system"]},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=800,
                temperature=0.5
            )

            # æå–å“åº”æ–‡æœ¬
            return response.choices[0].message.content

        except Exception as e:
            logger.error(f"AI APIè°ƒç”¨å¤±è´¥: {str(e)}")
            return f"AIåˆ†æå¤±è´¥: {str(e)}"
    def _structure_ai_response(self, response: str, analysis_type: str) -> Dict[str, Any]:
        """å°†AIå“åº”ç»“æ„åŒ–"""
        # å°è¯•ä»å“åº”ä¸­æå–ç»“æ„åŒ–å†…å®¹
        structured = {
            "title": f"{analysis_type.replace('_', ' ').title()}",
            "summary": "",
            "highlights": [],
            "metrics": {},
            "details": response
        }

        # ç®€å•çš„æ–‡æœ¬è§£æï¼Œæå–æ‘˜è¦å’Œè¦ç‚¹
        lines = response.split('\n')
        summary_found = False
        highlights_section = False

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # è¯†åˆ«æ‘˜è¦éƒ¨åˆ†
            if 'æ‘˜è¦' in line or 'Summary' in line.lower() or 'æ€»ç»“' in line:
                summary_found = True
                continue

            if summary_found and not structured["summary"] and line:
                structured["summary"] = line[:200]  # é™åˆ¶æ‘˜è¦é•¿åº¦
                summary_found = False

            # è¯†åˆ«è¦ç‚¹éƒ¨åˆ†
            if 'è¦ç‚¹' in line or 'highlights' in line.lower() or 'å…³é”®å‘ç°' in line:
                highlights_section = True
                continue

            if highlights_section and line.startswith(('â€¢', '-', '*', '1', '2', '3')):
                highlight = line.lstrip('â€¢-*123456789. ')[:50]
                if highlight and len(structured["highlights"]) < 5:
                    structured["highlights"].append(highlight)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ‘˜è¦ï¼Œç”Ÿæˆä¸€ä¸ª
        if not structured["summary"]:
            structured["summary"] = response[:150].split('.')[0] + '.'

        # å¦‚æœæ²¡æœ‰è¦ç‚¹ï¼Œä»è¯¦æƒ…ä¸­æå–
        if not structured["highlights"] and len(response) > 100:
            sentences = response.split('ã€‚')[:3]
            structured["highlights"] = [s.strip()[:50] for s in sentences if s.strip()]

        return structured

    def analyze_trajectory_data(self,
                                trajectory_json: str,
                                geocoded_json: str = None,
                                output_path: str = "analysis_results.json") -> Dict[str, Any]:
        """
        åˆ†æè½¨è¿¹æ•°æ®

        Args:
            trajectory_json: è½¨è¿¹JSONæ–‡ä»¶è·¯å¾„
            geocoded_json: åœ°ç†ç¼–ç JSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        # åŠ è½½æ•°æ®
        logger.info("åŠ è½½è½¨è¿¹æ•°æ®...")
        with open(trajectory_json, 'r', encoding='utf-8') as f:
            traj_data = json.load(f)

        geocoded_data = None
        if geocoded_json and os.path.exists(geocoded_json):
            logger.info("åŠ è½½åœ°ç†ç¼–ç æ•°æ®...")
            with open(geocoded_json, 'r', encoding='utf-8') as f:
                geocoded_data = json.load(f)

        results = {}
        detailed_results = {}  # å­˜å‚¨è¯¦ç»†ç»“æœ

        # å¯¹æ¯ä¸ªç”¨æˆ·è¿›è¡Œåˆ†æ
        for user_data in traj_data.get('users', []):
            user_id = user_data['user_id']
            logger.info(f"åˆ†æç”¨æˆ· {user_id}...")

            # ä¸ºç”¨æˆ·åŒ¹é…åœ°ç†ç¼–ç æ•°æ®
            user_geocoded = self._extract_user_geocoded_data(user_id, geocoded_data)

            # æ‰§è¡Œå„ç±»åˆ†æ
            user_results = {}
            user_detailed = {}

            # åˆ†ç¦»meta_synthesiså’Œå…¶ä»–åˆ†æç±»å‹
            regular_analyses = [a for a in self.config.analysis_types if a != "meta_synthesis"]
            has_meta_synthesis = "meta_synthesis" in self.config.analysis_types

            # æ‰§è¡Œå¸¸è§„åˆ†æ
            for analysis_type in regular_analyses:
                logger.info(f"  æ‰§è¡Œ {analysis_type} åˆ†æ...")
                try:
                    result = self._run_single_analysis(
                        analysis_type, user_data, user_geocoded
                    )

                    # åˆ†ç¦»æ‘˜è¦å’Œè¯¦ç»†ç»“æœ
                    summary_result = {
                        "analysis_type": result["analysis_type"],
                        "timestamp": result["timestamp"],
                        "summary": result["structured_result"]["summary"],
                        "highlights": result["structured_result"]["highlights"],
                        "metrics": result["structured_result"]["metrics"]
                    }
                    user_results[analysis_type] = summary_result

                    # ä¿å­˜è¯¦ç»†ç»“æœ
                    if self.config.save_detailed_separately:
                        user_detailed[analysis_type] = result

                    # æ·»åŠ å»¶è¿Ÿé¿å…APIè¯·æ±‚è¿‡å¿«
                    time.sleep(0.5)

                except Exception as e:
                    logger.error(f"åˆ†æ {analysis_type} å¤±è´¥: {str(e)}")
                    user_results[analysis_type] = {"error": str(e)}

            # å¦‚æœå¯ç”¨äº†meta_synthesisï¼Œæœ€åæ‰§è¡Œç»¼åˆåˆ†æ
            if has_meta_synthesis:
                logger.info(f"  æ‰§è¡Œ meta_synthesis ç»¼åˆåˆ†æ...")
                try:
                    # å‡†å¤‡æ‰€æœ‰å·²å®Œæˆåˆ†æçš„ç»“æœ
                    meta_result = self._run_meta_synthesis(
                        user_id, user_detailed, user_data, user_geocoded
                    )

                    # åˆ†ç¦»æ‘˜è¦å’Œè¯¦ç»†ç»“æœ
                    summary_result = {
                        "analysis_type": "meta_synthesis",
                        "timestamp": meta_result.get("timestamp"),
                        "based_on_analyses": meta_result.get("based_on_analyses", []),
                        "processing_time": meta_result.get("processing_time", 0),
                        "content": meta_result.get("content", "")[:500] + "..." if len(
                            meta_result.get("content", "")) > 500 else meta_result.get("content", "")
                    }
                    user_results["meta_synthesis"] = summary_result

                    # ä¿å­˜è¯¦ç»†ç»“æœ
                    if self.config.save_detailed_separately:
                        user_detailed["meta_synthesis"] = meta_result

                except Exception as e:
                    logger.error(f"  meta_synthesis åˆ†æå¤±è´¥: {str(e)}")
                    import traceback
                    traceback.print_exc()
                    user_results["meta_synthesis"] = {"error": str(e)}

            results[user_id] = user_results
            if self.config.save_detailed_separately:
                detailed_results[user_id] = user_detailed

        # ä¿å­˜æ‘˜è¦ç»“æœ
        output_data = {
            "analysis_timestamp": datetime.now().isoformat(),
            "config": {
                "model_name": self.config.model_name,
                "output_level": self.config.output_level,
                "analysis_types": self.config.analysis_types
            },
            "results": results
        }
        detailed_path = None
        md_path = None
        logger.info(f"ä¿å­˜æ‘˜è¦ç»“æœåˆ°: {output_path}")
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        # ä¿å­˜è¯¦ç»†ç»“æœåˆ°å•ç‹¬æ–‡ä»¶
        if self.config.save_detailed_separately and detailed_results:
            detailed_path = output_path.replace('.json', '_detailed.json')
            logger.info(f"ä¿å­˜è¯¦ç»†ç»“æœåˆ°: {detailed_path}")
            with open(detailed_path, 'w', encoding='utf-8') as f:
                json.dump({
                    "analysis_timestamp": datetime.now().isoformat(),
                    "config": {
                        "model_name": self.config.model_name,
                        "output_level": self.config.output_level
                    },
                    "detailed_results": detailed_results
                }, f, ensure_ascii=False, indent=2)

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        if self.config.generate_markdown_report:
            md_path = output_path.replace('.json', '.md')
            logger.info(f"ç”ŸæˆMarkdownæŠ¥å‘Š: {md_path}")
            self._generate_markdown_report(output_data, md_path)
        logger.info("=" * 50)
        logger.info(f"âœ… åˆ†æå®Œæˆï¼")
        logger.info(f"ğŸ“Š æ‘˜è¦ç»“æœ: {output_path}")
        if self.config.save_detailed_separately and detailed_results:
            logger.info(f"ğŸ“‹ è¯¦ç»†ç»“æœ: {detailed_path}")
        if self.config.generate_markdown_report:
            logger.info(f"ğŸ“„ MarkdownæŠ¥å‘Š: {md_path}")
        logger.info("=" * 50)

        return output_data

    def _generate_markdown_report(self, data: Dict, output_path: str):
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("# è½¨è¿¹æ•°æ®AIåˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {data['analysis_timestamp']}\n")
            f.write(f"**ä½¿ç”¨æ¨¡å‹**: {data['config']['model_name']}\n\n")

            for user_id, user_results in data['results'].items():
                f.write(f"## ç”¨æˆ· {user_id}\n\n")

                for analysis_type, result in user_results.items():
                    if 'error' in result:
                        f.write(f"### âŒ {analysis_type.replace('_', ' ').title()}\n")
                        f.write(f"é”™è¯¯: {result['error']}\n\n")
                        continue

                    f.write(f"### âœ… {analysis_type.replace('_', ' ').title()}\n\n")

                    # ç‰¹æ®Šå¤„ç† meta_synthesis
                    if analysis_type == "meta_synthesis":
                        if 'content' in result and result['content']:
                            # å¦‚æœå†…å®¹è¢«æˆªæ–­ï¼Œå°è¯•è·å–å®Œæ•´å†…å®¹
                            content = result['content']
                            if content.endswith('...'):
                                f.write(f"**ç»¼åˆåˆ†ææ‘˜è¦**:\n{content}\n\n")
                                f.write("*æ³¨ï¼šå®Œæ•´åˆ†æè¯·æŸ¥çœ‹è¯¦ç»†ç»“æœæ–‡ä»¶*\n\n")
                            else:
                                f.write(f"{content}\n\n")

                        if 'based_on_analyses' in result:
                            f.write(f"**åŸºäºåˆ†æç±»å‹**: {', '.join(result['based_on_analyses'])}\n\n")
                    else:
                        # åŸæœ‰é€»è¾‘å¤„ç†å…¶ä»–åˆ†æç±»å‹
                        if 'summary' in result and result['summary']:
                            f.write(f"**æ‘˜è¦**: {result['summary']}\n\n")

                        if 'highlights' in result and result['highlights']:
                            f.write("**å…³é”®å‘ç°**:\n")
                            for highlight in result['highlights']:
                                f.write(f"- {highlight}\n")
                            f.write("\n")

                        if 'metrics' in result and result['metrics']:
                            f.write("**å…³é”®æŒ‡æ ‡**:\n")
                            for key, value in result['metrics'].items():
                                f.write(f"- {key}: {value}\n")
                            f.write("\n")

                    f.write("---\n\n")
    def _extract_user_geocoded_data(self, user_id: str, geocoded_data: Optional[Dict]) -> Optional[List[Dict]]:
        """æå–ç‰¹å®šç”¨æˆ·çš„åœ°ç†ç¼–ç æ•°æ®"""
        if not geocoded_data:
            return None
        
        user_results = []
        for result in geocoded_data.get('results', []):
            if result.get('additional_info', {}).get('user_id') == user_id:
                user_results.append(result)
        
        return user_results if user_results else None

    def _run_single_analysis(self,
                             analysis_type: str,
                             user_data: Dict,
                             geocoded_data: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """è¿è¡Œå•é¡¹åˆ†æ"""

        # å‡†å¤‡åˆ†ææ•°æ®
        analysis_data = self._prepare_analysis_data(analysis_type, user_data, geocoded_data)

        # æ„å»ºprompt
        prompt = self._build_prompt(analysis_type, analysis_data)

        # è°ƒç”¨AI API
        response = self._call_ai_api(prompt)

        # ç»“æ„åŒ–å“åº”
        structured_result = self._structure_ai_response(response, analysis_type)

        # æ ¹æ®è¾“å‡ºçº§åˆ«å¤„ç†ç»“æœ
        result = {
            "analysis_type": analysis_type,
            "timestamp": datetime.now().isoformat(),
            "structured_result": structured_result,
            "data_summary": self._get_data_summary(user_data)
        }

        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦åŒ…å«å®Œæ•´å“åº”
        if self.config.output_level == "detailed":
            result["raw_response"] = response
        elif self.config.output_level == "standard":
            result["raw_response"] = response[:self.config.max_preview_length] + "..." if len(
                response) > self.config.max_preview_length else response
        # summaryçº§åˆ«ä¸åŒ…å«raw_response

        return result
    def _prepare_analysis_data(self, 
                               analysis_type: str, 
                               user_data: Dict, 
                               geocoded_data: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """å‡†å¤‡ç‰¹å®šåˆ†æç±»å‹çš„æ•°æ®"""
        
        base_data = {
            "n_points": user_data.get('n_points', 0),
            "time_metrics": json.dumps(user_data.get('time_metrics', {}), indent=2),
            "space_metrics": json.dumps(user_data.get('space_metrics', {}), indent=2),
            "spatiotemporal_metrics": json.dumps(user_data.get('spatiotemporal_metrics', {}), indent=2)
        }
        
        # å¤„ç†åœç•™ç‚¹ä¿¡æ¯
        stays = user_data.get('stays', [])
        stays_summary = self._summarize_stays(stays[:10])
        base_data["stays_summary"] = stays_summary
        
        # å¤„ç†å‡ºè¡Œä¿¡æ¯
        trips = user_data.get('trips', [])
        trips_summary = self._summarize_trips(trips[:10])
        base_data["trips_summary"] = trips_summary
        
        # æ—¶é—´è·¨åº¦
        if stays:
            time_span = f"ä» {stays[0].get('t_start', 'N/A')} åˆ° {stays[-1].get('t_end', 'N/A')}"
        else:
            time_span = "æ— æ³•ç¡®å®š"
        base_data["time_span"] = time_span
        
        # ç»Ÿè®¡ä¿¡æ¯
        base_data.update({
            "n_stays": len(stays),
            "n_trips": len(trips)
        })
        
        # åœ°ç†ç¼–ç æ ·æœ¬
        if geocoded_data:
            geocoded_sample = json.dumps(geocoded_data[:3], indent=2, ensure_ascii=False)
            base_data["geocoded_sample"] = geocoded_sample
            base_data["geocoded_context"] = self._extract_geocoded_context(geocoded_data)
        else:
            base_data["geocoded_sample"] = "æ— åœ°ç†ç¼–ç æ•°æ®"
            base_data["geocoded_context"] = "æ— åœ°ç†ç¼–ç æ•°æ®"
        
        # æ ¹æ®åˆ†æç±»å‹æä¾›ç‰¹å®šæ•°æ®
        if analysis_type == "temporal_comparative":
            base_data["temporal_data"] = self._prepare_comparative_temporal_data(user_data)
        elif analysis_type == "spatial_differential":
            base_data["spatial_data"] = self._prepare_differential_spatial_data(user_data)
            base_data["geocoded_context"] = self._extract_geocoded_context(
                geocoded_data) if geocoded_data else "æ— åœ°ç†ç¼–ç ä¸Šä¸‹æ–‡"
        elif analysis_type == "spatiotemporal_transitions":
            base_data.update({
                "comprehensive_data": self._prepare_comprehensive_data(user_data, geocoded_data),
                "stays_summary": self._summarize_stays(user_data.get('stays', [])),
                "trips_summary": self._summarize_trips(user_data.get('trips', []))
            })
        elif analysis_type == "cross_feature_insights":
            base_data.update({
                "comprehensive_data": self._prepare_comprehensive_data(user_data, geocoded_data),
                "pattern_data": self._prepare_pattern_data(user_data)
            })
        elif analysis_type == "anomaly_explanatory":
            base_data["pattern_data"] = self._prepare_pattern_data(user_data)
        elif analysis_type == "meta_synthesis":
            # meta_synthesisçš„æ•°æ®å‡†å¤‡åœ¨_run_meta_synthesisä¸­å•ç‹¬å¤„ç†
            pass
        
        return base_data
    
    def _summarize_stays(self, stays: List[Dict]) -> str:
        """æ€»ç»“åœç•™ç‚¹ä¿¡æ¯"""
        if not stays:
            return "æ— åœç•™ç‚¹æ•°æ®"
        
        summary = []
        for i, stay in enumerate(stays):
            duration_h = stay.get('duration_s', 0) / 3600
            summary.append(
                f"åœç•™ç‚¹{i+1}: ä½ç½®({stay.get('lat', 'N/A'):.4f}, {stay.get('lon', 'N/A'):.4f}), "
                f"æ—¶é•¿{duration_h:.1f}å°æ—¶, æ—¶é—´{stay.get('t_start', 'N/A')}"
            )
        
        return "\n".join(summary)
    
    def _summarize_trips(self, trips: List[Dict]) -> str:
        """æ€»ç»“å‡ºè¡Œä¿¡æ¯"""
        if not trips:
            return "æ— å‡ºè¡Œæ•°æ®"
        
        summary = []
        for i, trip in enumerate(trips):
            duration_h = trip.get('duration_s', 0) / 3600
            distance_km = trip.get('distance_m', 0) / 1000
            summary.append(
                f"å‡ºè¡Œ{i+1}: è·ç¦»{distance_km:.2f}km, æ—¶é•¿{duration_h:.1f}å°æ—¶, "
                f"èµ·ç‚¹({trip.get('start_lat', 'N/A'):.4f}, {trip.get('start_lon', 'N/A'):.4f}), "
                f"æ—¶é—´{trip.get('t_start', 'N/A')}"
            )
        
        return "\n".join(summary)
    
    def _prepare_spatial_data(self, user_data: Dict) -> str:
        """å‡†å¤‡ç©ºé—´åˆ†ææ•°æ®"""
        spatial = user_data.get('space_metrics', {})
        return json.dumps({
            "convex_hull_area_m2": spatial.get('hull_area_m2'),
            "radius_of_gyration_m": spatial.get('radius_of_gyration_m'),
            "ellipse_parameters": {
                "sx_m": spatial.get('ellipse_sx_m'),
                "sy_m": spatial.get('ellipse_sy_m'),
                "theta_deg": spatial.get('ellipse_theta_deg')
            },
            "kde_hotspots": spatial.get('kde_hotspots_utm32650', [])
        }, indent=2)

    def _prepare_comparative_temporal_data(self, user_data: Dict) -> str:
        """å‡†å¤‡å¯¹æ¯”æ—¶é—´åˆ†ææ•°æ®"""
        temporal = user_data.get('time_metrics', {})
        trips = user_data.get('trips', [])

        # è§£ææ—¶é—´æˆ³ï¼ŒåŒºåˆ†å·¥ä½œæ—¥å’Œå‘¨æœ«
        weekday_trips = []
        weekend_trips = []

        for trip in trips:
            try:
                # è§£æISO 8601æ ¼å¼æ—¶é—´æˆ³ (å¦‚: "2008-10-23T10:53:04.000001+08:00")
                trip_time = pd.to_datetime(trip.get('t_start', ''))
                weekday = trip_time.weekday()

                if weekday < 5:  # 0-4ä¸ºå‘¨ä¸€åˆ°å‘¨äº”
                    weekday_trips.append(trip)
                else:  # 5-6ä¸ºå‘¨å…­å‘¨æ—¥
                    weekend_trips.append(trip)
            except Exception as e:
                logger.debug(f"æ—¶é—´è§£æå¤±è´¥: {trip.get('t_start', 'N/A')}, é”™è¯¯: {e}")
                continue

        # åŒæ—¶å¤„ç†staysæ•°æ®ç”¨äºæ›´å…¨é¢çš„å¯¹æ¯”åˆ†æ
        stays = user_data.get('stays', [])
        weekday_stays = []
        weekend_stays = []

        for stay in stays:
            try:
                stay_time = pd.to_datetime(stay.get('t_start', ''))
                if stay_time.weekday() < 5:
                    weekday_stays.append(stay)
                else:
                    weekend_stays.append(stay)
            except:
                continue

        # è®¡ç®—å¯¹æ¯”æŒ‡æ ‡ - æ‰©å±•ç‰ˆ
        weekday_trip_count = len(weekday_trips)
        weekend_trip_count = len(weekend_trips)
        weekday_stay_count = len(weekday_stays)
        weekend_stay_count = len(weekend_stays)

        # è®¡ç®—å·®å€¼å’Œå˜åŒ–ç‡
        trip_count_diff = weekend_trip_count - weekday_trip_count / 5 * 2 if weekday_trip_count > 0 else weekend_trip_count
        trip_count_change_pct = (trip_count_diff / max(weekday_trip_count / 5 * 2,
                                                       1)) * 100 if weekday_trip_count > 0 else 0

        # è®¡ç®—å¹³å‡å‡ºè¡Œæ—¶é•¿å¯¹æ¯”
        weekday_trip_durations = [trip.get('duration_s', 0) for trip in weekday_trips]
        weekend_trip_durations = [trip.get('duration_s', 0) for trip in weekend_trips]

        weekday_avg_duration = np.mean(weekday_trip_durations) if weekday_trip_durations else 0
        weekend_avg_duration = np.mean(weekend_trip_durations) if weekend_trip_durations else 0
        duration_diff = weekend_avg_duration - weekday_avg_duration

        # è®¡ç®—æ´»è·ƒæ—¶æ®µå³°å€¼å·®å¼‚
        weekday_hours = [pd.to_datetime(trip.get('t_start', '')).hour for trip in weekday_trips if trip.get('t_start')]
        weekend_hours = [pd.to_datetime(trip.get('t_start', '')).hour for trip in weekend_trips if trip.get('t_start')]

        weekday_peak_hour = max(set(weekday_hours), key=weekday_hours.count) if weekday_hours else None
        weekend_peak_hour = max(set(weekend_hours), key=weekend_hours.count) if weekend_hours else None
        peak_hour_shift = (weekend_peak_hour - weekday_peak_hour) if (weekday_peak_hour and weekend_peak_hour) else 0

        return json.dumps({
            "temporal_comparison": {
                "weekday_vs_weekend": {
                    "trip_count": {
                        "weekday_total": weekday_trip_count,
                        "weekend_total": weekend_trip_count,
                        "weekday_daily_avg": weekday_trip_count / 5 if weekday_trip_count > 0 else 0,
                        "weekend_daily_avg": weekend_trip_count / 2 if weekend_trip_count > 0 else 0,
                        "difference": trip_count_diff,
                        "change_percentage": round(trip_count_change_pct, 2)
                    },
                    "trip_duration": {
                        "weekday_avg_seconds": round(weekday_avg_duration, 2),
                        "weekend_avg_seconds": round(weekend_avg_duration, 2),
                        "difference_seconds": round(duration_diff, 2),
                        "change_percentage": round((duration_diff / max(weekday_avg_duration, 1)) * 100,
                                                   2) if weekday_avg_duration > 0 else 0
                    },
                    "activity_peak": {
                        "weekday_peak_hour": weekday_peak_hour,
                        "weekend_peak_hour": weekend_peak_hour,
                        "peak_shift_hours": peak_hour_shift
                    },
                    "stay_pattern": {
                        "weekday_stay_count": weekday_stay_count,
                        "weekend_stay_count": weekend_stay_count
                    }
                }
            },
            "base_temporal_data": {
                "total_trip_count": temporal.get('n_trips'),
                "hourly_distribution": temporal.get('trip_start_hist_24h'),
                "time_entropy_normalized": temporal.get('time_entropy_hourly_norm'),
                "day_night_ratio": temporal.get('day_night_ratio')
            }
        }, indent=2, default=float)

    def _prepare_differential_spatial_data(self, user_data: Dict) -> str:
        """å‡†å¤‡å·®åˆ†ç©ºé—´åˆ†ææ•°æ®"""
        spatial = user_data.get('space_metrics', {})
        stays = user_data.get('stays', [])

        # è®¡ç®—çƒ­ç‚¹é‡å¿ƒï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”åŸºäºå¯†åº¦åˆ†æï¼‰
        if stays:
            lats = [stay.get('lat', 0) for stay in stays if stay.get('lat')]
            lons = [stay.get('lon', 0) for stay in stays if stay.get('lon')]

            if lats and lons:
                center_lat = np.mean(lats)
                center_lon = np.mean(lons)
                max_distance = max([
                    np.sqrt((lat - center_lat) ** 2 + (lon - center_lon) ** 2)
                    for lat, lon in zip(lats, lons)
                ]) if len(lats) > 1 else 0
            else:
                center_lat = center_lon = max_distance = 0
        else:
            center_lat = center_lon = max_distance = 0

        return json.dumps({
            "spatial_differential": {
                "hotspot_center": {
                    "lat": center_lat,
                    "lon": center_lon
                },
                "max_deviation_distance": max_distance,
                "spatial_concentration_index": 1 / (1 + max_distance) if max_distance > 0 else 1
            },
            "base_spatial_data": {
                "convex_hull_area_m2": spatial.get('hull_area_m2'),
                "radius_of_gyration_m": spatial.get('radius_of_gyration_m'),
                "ellipse_parameters": {
                    "sx_m": spatial.get('ellipse_sx_m'),
                    "sy_m": spatial.get('ellipse_sy_m'),
                    "theta_deg": spatial.get('ellipse_theta_deg')
                }
            }
        }, indent=2, default=float)
    
    def _prepare_comprehensive_data(self, user_data: Dict, geocoded_data: Optional[List[Dict]]) -> str:
        """å‡†å¤‡ç»¼åˆåˆ†ææ•°æ®"""
        data = {
            "trajectory_stats": user_data.get('time_metrics', {}),
            "spatial_patterns": user_data.get('space_metrics', {}),
            "stay_patterns": user_data.get('spatiotemporal_metrics', {}),
            "top_stays": user_data.get('spatiotemporal_metrics', {}).get('top5_stays', [])
        }
        
        if geocoded_data:
            poi_types = []
            for result in geocoded_data:
                pois = result.get('nearby_pois', [])
                poi_types.extend([poi.get('type', 'unknown') for poi in pois])
            
            poi_counts = {}
            for poi_type in poi_types:
                poi_counts[poi_type] = poi_counts.get(poi_type, 0) + 1
            
            data["poi_analysis"] = {
                "total_unique_pois": len(set(poi_types)),
                "poi_type_distribution": dict(sorted(poi_counts.items(), key=lambda x: x[1], reverse=True)[:10])
            }
        
        return json.dumps(data, indent=2, ensure_ascii=False)
    
    def _prepare_pattern_data(self, user_data: Dict) -> str:
        """å‡†å¤‡æ¨¡å¼æ£€æµ‹æ•°æ®"""
        trips = user_data.get('trips', [])
        stays = user_data.get('stays', [])
        
        if trips:
            trip_durations = [trip.get('duration_s', 0) for trip in trips]
            trip_distances = [trip.get('distance_m', 0) for trip in trips]
            
            trip_stats = {
                "duration_mean": np.mean(trip_durations),
                "duration_std": np.std(trip_durations),
                "distance_mean": np.mean(trip_distances),
                "distance_std": np.std(trip_distances),
                "trip_count": len(trips)
            }
        else:
            trip_stats = {}
        
        if stays:
            stay_durations = [stay.get('duration_s', 0) for stay in stays]
            stay_stats = {
                "duration_mean": np.mean(stay_durations),
                "duration_std": np.std(stay_durations),
                "stay_count": len(stays)
            }
        else:
            stay_stats = {}
        
        return json.dumps({
            "trip_statistics": trip_stats,
            "stay_statistics": stay_stats,
            "sample_trips": trips[:5],
            "sample_stays": stays[:5]
        }, indent=2, default=float)
    
    def _prepare_summary_data(self, user_data: Dict) -> str:
        """å‡†å¤‡æ€»ç»“æ•°æ®"""
        return json.dumps({
            "key_metrics": {
                "total_points": user_data.get('n_points'),
                "total_trips": user_data.get('time_metrics', {}).get('n_trips'),
                "total_stays": len(user_data.get('stays', [])),
                "activity_area_m2": user_data.get('space_metrics', {}).get('hull_area_m2'),
                "movement_radius_m": user_data.get('space_metrics', {}).get('radius_of_gyration_m')
            },
            "patterns_summary": {
                "time_regularity": user_data.get('time_metrics', {}).get('time_entropy_hourly_norm'),
                "spatial_concentration": "high" if user_data.get('space_metrics', {}).get('radius_of_gyration_m', 0) < 5000 else "low",
                "activity_balance": user_data.get('time_metrics', {}).get('day_night_ratio')
            }
        }, indent=2, default=float)
    
    def _extract_geocoded_context(self, geocoded_data: List[Dict]) -> str:
        """æå–åœ°ç†ç¼–ç ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        if not geocoded_data:
            return "æ— åœ°ç†ç¼–ç ä¸Šä¸‹æ–‡"
        
        contexts = []
        for data in geocoded_data[:5]:
            addr = data.get('formatted_address', 'N/A')
            pois = data.get('nearby_pois', [])
            poi_names = [poi.get('name', 'unknown') for poi in pois[:3]]
            
            contexts.append(f"åœ°å€: {addr}, é™„è¿‘POI: {', '.join(poi_names)}")
        
        return "\n".join(contexts)
    
    def _build_prompt(self, analysis_type: str, data: Dict[str, Any]) -> str:
        """æ„å»ºåˆ†ææç¤º"""
        task_prompt = self.prompt_templates.get(analysis_type, "")
        if not task_prompt:
            return ""
        
        # æ ¼å¼åŒ–ä»»åŠ¡æç¤º
        try:
            formatted_prompt = task_prompt.format(**data)
            return formatted_prompt
        except KeyError as e:
            logger.warning(f"Promptæ ¼å¼åŒ–ç¼ºå°‘é”®: {e}")
            return task_prompt
    
    def _parse_ai_response(self, response: str, analysis_type: str) -> Dict[str, Any]:
        """è§£æAIå“åº”"""
        return {
            "analysis_type": analysis_type,
            "content": response,
            "length": len(response),
            "timestamp": datetime.now().isoformat()
        }
    
    def _get_data_summary(self, user_data: Dict) -> Dict[str, Any]:
        """è·å–æ•°æ®æ‘˜è¦"""
        return {
            "user_id": user_data.get('user_id'),
            "n_points": user_data.get('n_points'),
            "n_stays": len(user_data.get('stays', [])),
            "n_trips": len(user_data.get('trips', [])),
            "has_labels": user_data.get('label_summary') is not None
        }

# ä¸»å‡½æ•°ç¤ºä¾‹
def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    import argparse

    parser = argparse.ArgumentParser(description='è½¨è¿¹æ•°æ®AIåˆ†æå™¨')
    parser.add_argument('-t', '--trajectory', required=True, help='è½¨è¿¹JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-g', '--geocoded', help='åœ°ç†ç¼–ç JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', default='analysis_results.json', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--level', choices=['summary', 'standard', 'detailed'],
                        default='summary', help='è¾“å‡ºè¯¦ç»†çº§åˆ«')
    parser.add_argument('--model', default='GPT-4o', help='ä½¿ç”¨çš„æ¨¡å‹åç§°')
    parser.add_argument('--no-markdown', action='store_true', help='ä¸ç”ŸæˆMarkdownæŠ¥å‘Š')
    parser.add_argument('--no-detailed', action='store_true', help='ä¸ä¿å­˜è¯¦ç»†ç»“æœæ–‡ä»¶')

    args = parser.parse_args()

    # é…ç½®
    config = AnalysisConfig(
        api_key=os.getenv("POE_API_KEY"),
        model_name=args.model,
        output_level=args.level,
        generate_markdown_report=not args.no_markdown,
        save_detailed_separately=not args.no_detailed and args.level != 'detailed',
        analysis_types=[
            "behavior_pattern",
            "mobility_summary",
            "spatial_analysis",
            "temporal_analysis",
            "lifestyle_inference",
            "recommendations"
        ]
    )

    if not config.api_key:
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ POE_API_KEY")
        return

    # åˆ›å»ºåˆ†æå™¨
    analyzer = TrajectoryAIAnalyzer(config)

    # åˆ†ææ•°æ®
    try:
        results = analyzer.analyze_trajectory_data(
            trajectory_json=args.trajectory,
            geocoded_json=args.geocoded,
            output_path=args.output
        )

        print(f"âœ… åˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š è¾“å‡ºçº§åˆ«: {args.level}")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶: {args.output}")

        if config.save_detailed_separately and args.level != 'detailed':
            print(f"ğŸ“ è¯¦ç»†ç»“æœ: {args.output.replace('.json', '_detailed.json')}")

        if config.generate_markdown_report:
            print(f"ğŸ“„ MarkdownæŠ¥å‘Š: {args.output.replace('.json', '.md')}")

    except Exception as e:
        logger.error(f"åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}")

if __name__ == "__main__":
    main()