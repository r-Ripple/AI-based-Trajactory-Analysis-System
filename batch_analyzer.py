#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡è½¨è¿¹æ•°æ®åˆ†æå™¨ - OpenAI SDKç‰ˆæœ¬
æ”¯æŒæ‰¹é‡å¤„ç†å¤šä¸ªç”¨æˆ·çš„è½¨è¿¹æ•°æ®
"""
import time
import os
import sys
import argparse
import json
import yaml
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

# å¯¼å…¥OpenAIç‰ˆæœ¬çš„åˆ†æå™¨
from trajectory_ai_analyzer import TrajectoryAIAnalyzer, AnalysisConfig

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class BatchAnalysisManager:
    """æ‰¹é‡åˆ†æç®¡ç†å™¨"""
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        åˆå§‹åŒ–æ‰¹é‡åˆ†æç®¡ç†å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_path)
        self.output_dir = Path(self.config.get('output', {}).get('base_dir', 'analysis_output'))
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def _load_config(self, config_path: str) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)

            # éªŒè¯å¿…è¦çš„é…ç½®é¡¹
            if 'api' not in config:
                raise ValueError("é…ç½®æ–‡ä»¶ç¼ºå°‘ 'api' éƒ¨åˆ†")

            # å…¼å®¹æ€§å¤„ç†ï¼šbot_name -> model_name
            if 'bot_name' in config['api'] and 'model_name' not in config['api']:
                logger.info("æ£€æµ‹åˆ° bot_nameï¼Œè‡ªåŠ¨è½¬æ¢ä¸º model_name")
                config['api']['model_name'] = config['api'].pop('bot_name')

            # å¦‚æœæ²¡æœ‰model_nameï¼Œä½¿ç”¨é»˜è®¤å€¼
            if 'model_name' not in config['api']:
                logger.warning("é…ç½®æ–‡ä»¶ç¼ºå°‘ model_nameï¼Œä½¿ç”¨é»˜è®¤å€¼ GPT-4o")
                config['api']['model_name'] = 'GPT-4o'

            # è®¾ç½®é»˜è®¤è¾“å‡ºé…ç½®
            if 'output' not in config:
                config['output'] = {}

            output_defaults = {
                'base_dir': 'analysis_output',
                'output_level': 'summary',
                'max_preview_length': 500,
                'save_detailed_separately': True,
                'generate_markdown_report': True,
                'include_preview': False  # é»˜è®¤å…³é—­é¢„è§ˆ
            }

            for key, value in output_defaults.items():
                if key not in config['output']:
                    config['output'][key] = value

            return config
        except FileNotFoundError:
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            sys.exit(1)
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
            sys.exit(1)

    def run_batch_analysis(self,
                           trajectory_file: str,
                           geocoded_file: Optional[str] = None,
                           output_name: Optional[str] = None) -> str:
        """è¿è¡Œæ‰¹é‡åˆ†æ"""
        logger.info("=" * 50)
        logger.info("æ‰¹é‡è½¨è¿¹æ•°æ®AIåˆ†æ")
        logger.info("=" * 50)

        # è·å–APIé…ç½®
        api_config = self.config.get('api', {})
        api_key = api_config.get('api_key') or os.getenv('POE_API_KEY')

        if not api_key:
            logger.error("æœªæ‰¾åˆ°APIå¯†é’¥ï¼")
            logger.error("è¯·åœ¨config.yamlä¸­è®¾ç½®æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ POE_API_KEY")
            sys.exit(1)

        # è·å–è¾“å‡ºé…ç½®
        output_config = self.config.get('output', {})

        # åˆ›å»ºåˆ†æé…ç½®
        analysis_config = AnalysisConfig(
            api_key=api_key,
            model_name=api_config.get('model_name', 'GPT-4o'),
            max_tokens=api_config.get('max_tokens', 4000),
            temperature=api_config.get('temperature', 0.7),
            analysis_types=self.config.get('analysis', {}).get('enabled_types', None),
            output_level=output_config.get('output_level', 'summary'),
            max_preview_length=output_config.get('max_preview_length', 500),
            save_detailed_separately=output_config.get('save_detailed_separately', True),
            generate_markdown_report=output_config.get('generate_markdown_report', True)
        )

        logger.info(f"æ¨¡å‹: {analysis_config.model_name} | è¾“å‡ºçº§åˆ«: {analysis_config.output_level}")

        # åˆ›å»ºåˆ†æå™¨
        analyzer = TrajectoryAIAnalyzer(analysis_config)

        # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
        if output_name is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_name = f"analysis_{timestamp}.json"

        output_path = self.output_dir / output_name

        # è¿è¡Œåˆ†æ
        try:
            results = analyzer.analyze_trajectory_data(
                trajectory_json=trajectory_file,
                geocoded_json=geocoded_file,
                output_path=str(output_path)
            )

            # ç”Ÿæˆç®€æ´æ‘˜è¦æŠ¥å‘Š
            summary_path = self.output_dir / f"summary_{output_name.replace('.json', '.txt')}"
            self._generate_summary_report(results, summary_path)

            logger.info("=" * 50)
            logger.info("âœ… åˆ†æå®Œæˆ")
            logger.info(f"æ‘˜è¦: {output_path}")
            if analysis_config.save_detailed_separately:
                logger.info(f"è¯¦ç»†: {str(output_path).replace('.json', '_detailed.json')}")
            if analysis_config.generate_markdown_report:
                logger.info(f"æŠ¥å‘Š: {str(output_path).replace('.json', '.md')}")
            logger.info("=" * 50)

            return str(output_path)

        except Exception as e:
            logger.error(f"æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            sys.exit(1)

    def _generate_summary_report(self, results: Dict, output_path: Path):
        """ç”Ÿæˆç®€æ´æ‘˜è¦æŠ¥å‘Š"""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("è½¨è¿¹AIåˆ†ææ‘˜è¦\n")
            f.write(f"æ—¶é—´: {results['analysis_timestamp'][:19]}\n")
            f.write(f"æ¨¡å‹: {results['config']['model_name']}\n")
            f.write("-" * 40 + "\n")

            for user_id, user_results in results['results'].items():
                f.write(f"\nç”¨æˆ· {user_id}:\n")

                success_count = sum(1 for r in user_results.values() if 'error' not in r)
                total_count = len(user_results)
                f.write(f"  å®Œæˆ: {success_count}/{total_count}\n")

                # åªæ˜¾ç¤ºæ‘˜è¦ä¿¡æ¯
                for analysis_type, result in user_results.items():
                    if 'error' in result:
                        f.write(f"  âœ— {analysis_type[:20]}: å¤±è´¥\n")
                    else:
                        summary = result.get('summary', '')[:80]
                        if summary:
                            f.write(f"  âœ“ {analysis_type[:20]}: {summary}\n")
                        else:
                            f.write(f"  âœ“ {analysis_type[:20]}: å®Œæˆ\n")


def create_default_config():
    """åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶"""
    default_config = {
        'api': {
            'model_name': 'GPT-4o',
            'max_tokens': 4000,
            'temperature': 0.7,
            'api_key': None
        },
        'analysis': {
            'enabled_types': [
    'temporal_comparative',           # å¿…é€‰1ï¼šæ—¶é—´å¯¹æ¯”åˆ†æ
    'spatial_differential',          # å¿…é€‰2ï¼šç©ºé—´å·®åˆ†åˆ†æ
    'spatiotemporal_transitions',    # å¿…é€‰3ï¼šæ—¶ç©ºè½¬åœºä¸é“¾æ¡
    'cross_feature_insights',        # å¿…é€‰4ï¼šè·¨ç»´å…³è”åˆ†æ
    'anomaly_explanatory'            # å¯é€‰ï¼šè§£é‡Šæ€§å¼‚å¸¸æ£€æµ‹
    'meta_synthesis'  # æœ€ç»ˆï¼šç»¼åˆæ´å¯Ÿæ±‡æ€»
]
        },
        'output': {
            'base_dir': 'analysis_output1',
            'output_level': 'summary',  # summary/standard/detailed
            'max_preview_length': 500,
            'save_detailed_separately': True,
            'generate_markdown_report': True,
            'include_preview': False
        }
    }

    with open('config.yaml', 'w', encoding='utf-8') as f:
        yaml.dump(default_config, f, allow_unicode=True, default_flow_style=False)

    print("âœ… å·²åˆ›å»ºé…ç½®æ–‡ä»¶: config.yaml")
    print("\né…ç½®è¯´æ˜:")
    print("output_levelé€‰é¡¹:")
    print("  - summary: ä»…ä¿å­˜æ‘˜è¦å’Œè¦ç‚¹")
    print("  - standard: åŒ…å«é¢„è§ˆ(å‰500å­—ç¬¦)")
    print("  - detailed: åŒ…å«å®Œæ•´å“åº”")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ‰¹é‡è½¨è¿¹æ•°æ®åˆ†æç®¡ç†å™¨')
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # initå‘½ä»¤ - åˆå§‹åŒ–é…ç½®
    init_parser = subparsers.add_parser('init', help='åˆå§‹åŒ–é…ç½®æ–‡ä»¶')
    init_parser.add_argument(
        '-o', '--output',
        default='config.yaml',
        help='é…ç½®æ–‡ä»¶è¾“å‡ºè·¯å¾„ï¼ˆé»˜è®¤: config.yamlï¼‰'
    )

    # analyzeå‘½ä»¤ - è¿è¡Œåˆ†æ
    analyze_parser = subparsers.add_parser('analyze', help='è¿è¡Œæ‰¹é‡åˆ†æ')
    analyze_parser.add_argument(
        '-t', '--trajectory',
        required=True,
        help='è½¨è¿¹æ•°æ®JSONæ–‡ä»¶è·¯å¾„'
    )
    analyze_parser.add_argument(
        '-g', '--geocoded',
        help='åœ°ç†ç¼–ç JSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰'
    )
    analyze_parser.add_argument(
        '-c', '--config',
        default='config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: config.yamlï¼‰'
    )
    analyze_parser.add_argument(
        '-o', '--output',
        help='è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æ—¶é—´æˆ³ï¼‰'
    )
    # æ–°å¢å‘½ä»¤è¡Œå‚æ•°
    analyze_parser.add_argument(
        '--level',
        choices=['summary', 'standard', 'detailed'],
        help='è¦†ç›–é…ç½®æ–‡ä»¶çš„è¾“å‡ºçº§åˆ«è®¾ç½®'
    )
    analyze_parser.add_argument(
        '--model',
        help='è¦†ç›–é…ç½®æ–‡ä»¶çš„æ¨¡å‹è®¾ç½®'
    )
    analyze_parser.add_argument(
        '--types',
        nargs='+',
        help='æŒ‡å®šè¦è¿è¡Œçš„åˆ†æç±»å‹ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰'
    )

    # listå‘½ä»¤ - åˆ—å‡ºåˆ†æç»“æœ
    list_parser = subparsers.add_parser('list', help='åˆ—å‡ºæ‰€æœ‰åˆ†æç»“æœ')
    list_parser.add_argument(
        '-d', '--dir',
        default='analysis_output',
        help='åˆ†æè¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: analysis_outputï¼‰'
    )
    list_parser.add_argument(
        '-n', '--limit',
        type=int,
        default=10,
        help='æ˜¾ç¤ºæœ€è¿‘çš„Nä¸ªç»“æœï¼ˆé»˜è®¤: 10ï¼‰'
    )

    # viewå‘½ä»¤ - æŸ¥çœ‹åˆ†æç»“æœ
    view_parser = subparsers.add_parser('view', help='æŸ¥çœ‹ç‰¹å®šåˆ†æç»“æœ')
    view_parser.add_argument(
        'file',
        help='è¦æŸ¥çœ‹çš„ç»“æœæ–‡ä»¶è·¯å¾„'
    )
    view_parser.add_argument(
        '-f', '--format',
        choices=['json', 'summary', 'markdown'],
        default='summary',
        help='æ˜¾ç¤ºæ ¼å¼ï¼ˆé»˜è®¤: summaryï¼‰'
    )

    # compareå‘½ä»¤ - å¯¹æ¯”åˆ†æç»“æœ
    compare_parser = subparsers.add_parser('compare', help='å¯¹æ¯”ä¸¤ä¸ªåˆ†æç»“æœ')
    compare_parser.add_argument(
        'file1',
        help='ç¬¬ä¸€ä¸ªç»“æœæ–‡ä»¶'
    )
    compare_parser.add_argument(
        'file2',
        help='ç¬¬äºŒä¸ªç»“æœæ–‡ä»¶'
    )

    # cleanå‘½ä»¤ - æ¸…ç†æ—§ç»“æœ
    clean_parser = subparsers.add_parser('clean', help='æ¸…ç†æ—§çš„åˆ†æç»“æœ')
    clean_parser.add_argument(
        '-d', '--dir',
        default='analysis_output',
        help='åˆ†æè¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: analysis_outputï¼‰'
    )
    clean_parser.add_argument(
        '-k', '--keep',
        type=int,
        default=5,
        help='ä¿ç•™æœ€è¿‘çš„Nä¸ªç»“æœï¼ˆé»˜è®¤: 5ï¼‰'
    )
    clean_parser.add_argument(
        '-y', '--yes',
        action='store_true',
        help='è·³è¿‡ç¡®è®¤ç›´æ¥åˆ é™¤'
    )

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        sys.exit(0)

    # æ‰§è¡Œå‘½ä»¤
    if args.command == 'init':
        # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
        default_config = {
            'api': {
                'base_url': 'https://api.poe.com/v1',
                'model_name': 'GPT-4o',
                'max_tokens': 4000,
                'temperature': 0.7,
                'api_key': None
            },
            'analysis': {
                'enabled_types': [
                    'behavior_pattern',
                    'mobility_summary',
                    'spatial_analysis',
                    'temporal_analysis',
                    'lifestyle_inference',
                    'recommendations'
                ],
                'batch_size': 5,
                'retry_count': 3,
                'retry_delay': 2
            },
            'output': {
                'base_dir': 'analysis_output',
                'output_level': 'summary',
                'max_preview_length': 500,
                'save_detailed_separately': True,
                'generate_markdown_report': True,
                'generate_summary_txt': True,
                'save_raw_responses': False
            },
            'data_processing': {
                'sample_size': None,
                'time_zone': 'UTC',
                'coordinate_precision': 6
            }
        }

        if os.path.exists(args.output):
            response = input(f"é…ç½®æ–‡ä»¶ {args.output} å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–ï¼Ÿ(y/n): ")
            if response.lower() != 'y':
                print("å·²å–æ¶ˆ")
                sys.exit(0)

        with open(args.output, 'w', encoding='utf-8') as f:
            yaml.dump(default_config, f, default_flow_style=False, allow_unicode=True)

        print(f"âœ… é…ç½®æ–‡ä»¶å·²åˆ›å»º: {args.output}")
        print("\nè¯·ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼Œè®¾ç½®æ‚¨çš„APIå¯†é’¥å’Œå…¶ä»–å‚æ•°")
        print("æ‚¨ä¹Ÿå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®APIå¯†é’¥: export POE_API_KEY=your_key_here")

    elif args.command == 'analyze':
        # æ£€æŸ¥è½¨è¿¹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(args.trajectory):
            logger.error(f"è½¨è¿¹æ–‡ä»¶ä¸å­˜åœ¨: {args.trajectory}")
            sys.exit(1)

        # æ£€æŸ¥åœ°ç†ç¼–ç æ–‡ä»¶ï¼ˆå¦‚æœæä¾›ï¼‰
        if args.geocoded and not os.path.exists(args.geocoded):
            logger.warning(f"åœ°ç†ç¼–ç æ–‡ä»¶ä¸å­˜åœ¨: {args.geocoded}")
            args.geocoded = None

        # æ£€æŸ¥é…ç½®æ–‡ä»¶
        if not os.path.exists(args.config):
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
            logger.info("ä½¿ç”¨ 'python batch_analyzer.py init' åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶")
            sys.exit(1)

        # åˆ›å»ºç®¡ç†å™¨
        manager = BatchAnalysisManager(args.config)

        # è¦†ç›–é…ç½®ï¼ˆå¦‚æœæä¾›äº†å‘½ä»¤è¡Œå‚æ•°ï¼‰
        if args.level:
            manager.config['output']['output_level'] = args.level
            logger.info(f"ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„è¾“å‡ºçº§åˆ«: {args.level}")

        if args.model:
            manager.config['api']['model_name'] = args.model
            logger.info(f"ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„æ¨¡å‹: {args.model}")

        if args.types:
            manager.config['analysis']['enabled_types'] = args.types
            logger.info(f"ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„åˆ†æç±»å‹: {', '.join(args.types)}")

        # è¿è¡Œåˆ†æ
        results_file = manager.run_batch_analysis(
            trajectory_file=args.trajectory,
            geocoded_file=args.geocoded,
            output_name=args.output
        )

        print(f"\nâœ… åˆ†æå®Œæˆï¼ç»“æœæ–‡ä»¶: {results_file}")

    elif args.command == 'list':
        # åˆ—å‡ºåˆ†æç»“æœ
        if not os.path.exists(args.dir):
            print(f"ç›®å½•ä¸å­˜åœ¨: {args.dir}")
            sys.exit(1)

        # è·å–æ‰€æœ‰JSONæ–‡ä»¶
        json_files = []
        for file in os.listdir(args.dir):
            if file.endswith('.json') and not file.endswith('_detailed.json'):
                file_path = os.path.join(args.dir, file)
                stat = os.stat(file_path)
                json_files.append({
                    'file': file,
                    'path': file_path,
                    'size': stat.st_size,
                    'modified': stat.st_mtime
                })

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        json_files.sort(key=lambda x: x['modified'], reverse=True)

        if not json_files:
            print(f"æ²¡æœ‰æ‰¾åˆ°åˆ†æç»“æœæ–‡ä»¶åœ¨: {args.dir}")
            sys.exit(0)

        # æ˜¾ç¤ºç»“æœ
        print(f"\nğŸ“Š æœ€è¿‘çš„åˆ†æç»“æœ (å…± {len(json_files)} ä¸ª):\n")
        print(f"{'åºå·':<4} {'æ–‡ä»¶å':<40} {'å¤§å°':<10} {'ä¿®æ”¹æ—¶é—´':<20}")
        print("-" * 80)

        for i, file_info in enumerate(json_files[:args.limit], 1):
            size_str = f"{file_info['size'] / 1024:.1f}KB"
            time_str = datetime.fromtimestamp(file_info['modified']).strftime('%Y-%m-%d %H:%M:%S')
            print(f"{i:<4} {file_info['file']:<40} {size_str:<10} {time_str:<20}")

    elif args.command == 'view':
        # æŸ¥çœ‹åˆ†æç»“æœ
        if not os.path.exists(args.file):
            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
            sys.exit(1)

        try:
            with open(args.file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            if args.format == 'json':
                print(json.dumps(data, indent=2, ensure_ascii=False))

            elif args.format == 'summary':
                print("\n" + "=" * 60)
                print(f"ğŸ“Š åˆ†æç»“æœæ‘˜è¦")
                print("=" * 60)

                # æ˜¾ç¤ºå…ƒæ•°æ®
                if 'metadata' in data:
                    meta = data['metadata']
                    print(f"\nğŸ“… åˆ†ææ—¶é—´: {meta.get('analysis_timestamp', 'N/A')}")
                    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {meta.get('model_used', 'N/A')}")
                    print(f"ğŸ“ è¾“å‡ºçº§åˆ«: {meta.get('output_level', 'N/A')}")

                    if 'data_info' in meta:
                        info = meta['data_info']
                        print(f"\nğŸ“ æ•°æ®ä¿¡æ¯:")
                        print(f"  - è½¨è¿¹ç‚¹æ•°: {info.get('trajectory_points', 'N/A')}")
                        print(f"  - æ—¶é—´èŒƒå›´: {info.get('date_range', 'N/A')}")
                        if info.get('has_geocoding'):
                            print(f"  - åŒ…å«åœ°ç†ç¼–ç : âœ“")

                # æ˜¾ç¤ºåˆ†æç»“æœæ‘˜è¦
                if 'analysis_results' in data:
                    print(f"\nğŸ“Š åˆ†æç±»å‹:")
                    for analysis_type, content in data['analysis_results'].items():
                        print(f"\n  [{analysis_type}]")
                        if isinstance(content, dict):
                            if 'summary' in content:
                                print(f"    {content['summary'][:200]}...")
                            elif 'preview' in content:
                                print(f"    {content['preview'][:200]}...")
                        else:
                            print(f"    {str(content)[:200]}...")

                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                if 'statistics' in data:
                    stats = data['statistics']
                    print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
                    print(f"  - æ€»åˆ†ææ•°: {stats.get('total_analyses', 'N/A')}")
                    print(f"  - æˆåŠŸæ•°: {stats.get('successful_analyses', 'N/A')}")
                    print(f"  - å¤±è´¥æ•°: {stats.get('failed_analyses', 'N/A')}")
                    print(f"  - æ€»è€—æ—¶: {stats.get('total_time', 'N/A')}ç§’")

            elif args.format == 'markdown':
                md_file = args.file.replace('.json', '.md')
                if os.path.exists(md_file):
                    with open(md_file, 'r', encoding='utf-8') as f:
                        print(f.read())
                else:
                    print(f"Markdownæ–‡ä»¶ä¸å­˜åœ¨: {md_file}")

        except json.JSONDecodeError as e:
            print(f"JSONè§£æé”™è¯¯: {e}")
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶é”™è¯¯: {e}")

    elif args.command == 'compare':
        # å¯¹æ¯”ä¸¤ä¸ªåˆ†æç»“æœ
        if not os.path.exists(args.file1):
            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {args.file1}")
            sys.exit(1)
        if not os.path.exists(args.file2):
            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {args.file2}")
            sys.exit(1)

        try:
            with open(args.file1, 'r', encoding='utf-8') as f:
                data1 = json.load(f)
            with open(args.file2, 'r', encoding='utf-8') as f:
                data2 = json.load(f)

            print("\n" + "=" * 60)
            print("ğŸ“Š åˆ†æç»“æœå¯¹æ¯”")
            print("=" * 60)

            # å¯¹æ¯”å…ƒæ•°æ®
            print("\nğŸ“‹ åŸºæœ¬ä¿¡æ¯å¯¹æ¯”:")
            print(f"{'é¡¹ç›®':<20} {'æ–‡ä»¶1':<25} {'æ–‡ä»¶2':<25}")
            print("-" * 70)

            # æå–å…ƒæ•°æ®
            meta1 = data1.get('metadata', {})
            meta2 = data2.get('metadata', {})

            items = [
                ('åˆ†ææ—¶é—´', meta1.get('analysis_timestamp', 'N/A'), meta2.get('analysis_timestamp', 'N/A')),
                ('ä½¿ç”¨æ¨¡å‹', meta1.get('model_used', 'N/A'), meta2.get('model_used', 'N/A')),
                ('è¾“å‡ºçº§åˆ«', meta1.get('output_level', 'N/A'), meta2.get('output_level', 'N/A')),
            ]

            for item, val1, val2 in items:
                print(f"{item:<20} {str(val1):<25} {str(val2):<25}")

            # å¯¹æ¯”æ•°æ®ä¿¡æ¯
            if 'data_info' in meta1 or 'data_info' in meta2:
                print("\nğŸ“ æ•°æ®ä¿¡æ¯å¯¹æ¯”:")
                info1 = meta1.get('data_info', {})
                info2 = meta2.get('data_info', {})

                data_items = [
                    ('è½¨è¿¹ç‚¹æ•°', info1.get('trajectory_points', 'N/A'), info2.get('trajectory_points', 'N/A')),
                    ('æ—¶é—´èŒƒå›´', info1.get('date_range', 'N/A'), info2.get('date_range', 'N/A')),
                    ('åŒ…å«åœ°ç†ç¼–ç ', 'âœ“' if info1.get('has_geocoding') else 'âœ—',
                     'âœ“' if info2.get('has_geocoding') else 'âœ—'),
                ]

                for item, val1, val2 in data_items:
                    diff_mark = " âš ï¸" if val1 != val2 else ""
                    print(f"{item:<20} {str(val1):<25} {str(val2):<25}{diff_mark}")

            # å¯¹æ¯”åˆ†æç±»å‹
            types1 = set(data1.get('analysis_results', {}).keys())
            types2 = set(data2.get('analysis_results', {}).keys())

            print("\nğŸ“Š åˆ†æç±»å‹å¯¹æ¯”:")
            common_types = types1 & types2
            only_in_1 = types1 - types2
            only_in_2 = types2 - types1

            if common_types:
                print(f"  å…±åŒåˆ†æ: {', '.join(common_types)}")
            if only_in_1:
                print(f"  ä»…åœ¨æ–‡ä»¶1: {', '.join(only_in_1)}")
            if only_in_2:
                print(f"  ä»…åœ¨æ–‡ä»¶2: {', '.join(only_in_2)}")

            # å¯¹æ¯”ç»Ÿè®¡ä¿¡æ¯
            if 'statistics' in data1 or 'statistics' in data2:
                print("\nğŸ“ˆ ç»Ÿè®¡å¯¹æ¯”:")
                stats1 = data1.get('statistics', {})
                stats2 = data2.get('statistics', {})

                stat_items = [
                    ('æ€»åˆ†ææ•°', stats1.get('total_analyses', 0), stats2.get('total_analyses', 0)),
                    ('æˆåŠŸæ•°', stats1.get('successful_analyses', 0), stats2.get('successful_analyses', 0)),
                    ('å¤±è´¥æ•°', stats1.get('failed_analyses', 0), stats2.get('failed_analyses', 0)),
                    ('æ€»è€—æ—¶(ç§’)', f"{stats1.get('total_time', 0):.2f}", f"{stats2.get('total_time', 0):.2f}"),
                ]

                for item, val1, val2 in stat_items:
                    print(f"{item:<20} {str(val1):<25} {str(val2):<25}")

        except Exception as e:
            print(f"å¯¹æ¯”åˆ†æé”™è¯¯: {e}")

    elif args.command == 'clean':
        # æ¸…ç†æ—§ç»“æœ
        if not os.path.exists(args.dir):
            print(f"ç›®å½•ä¸å­˜åœ¨: {args.dir}")
            sys.exit(1)

        # è·å–æ‰€æœ‰ç›¸å…³æ–‡ä»¶
        all_files = []
        for file in os.listdir(args.dir):
            file_path = os.path.join(args.dir, file)
            if os.path.isfile(file_path):
                # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†æç»“æœæ–‡ä»¶
                if file.endswith(('.json', '.md', '.txt')):
                    stat = os.stat(file_path)
                    all_files.append({
                        'path': file_path,
                        'name': file,
                        'modified': stat.st_mtime
                    })

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        all_files.sort(key=lambda x: x['modified'], reverse=True)

        # è¯†åˆ«è¦åˆ é™¤çš„æ–‡ä»¶ç»„
        files_to_delete = []
        result_groups = {}

        # æŒ‰åŸºç¡€åç§°åˆ†ç»„
        for file_info in all_files:
            base_name = file_info['name'].split('.')[0].replace('_detailed', '').replace('_summary', '')
            if base_name not in result_groups:
                result_groups[base_name] = []
            result_groups[base_name].append(file_info)

        # ä¿ç•™æœ€æ–°çš„Nç»„
        sorted_groups = sorted(result_groups.items(),
                               key=lambda x: max(f['modified'] for f in x[1]),
                               reverse=True)

        for i, (base_name, files) in enumerate(sorted_groups):
            if i >= args.keep:
                files_to_delete.extend(files)

        if not files_to_delete:
            print(f"æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ–‡ä»¶ï¼ˆä¿ç•™æœ€è¿‘ {args.keep} ä¸ªç»“æœï¼‰")
            sys.exit(0)

        # æ˜¾ç¤ºè¦åˆ é™¤çš„æ–‡ä»¶
        print(f"\nå°†åˆ é™¤ä»¥ä¸‹ {len(files_to_delete)} ä¸ªæ–‡ä»¶:")
        for file_info in files_to_delete:
            print(f"  - {file_info['name']}")

        # ç¡®è®¤åˆ é™¤
        if not args.yes:
            response = input(f"\nç¡®è®¤åˆ é™¤è¿™äº›æ–‡ä»¶ï¼Ÿ(y/n): ")
            if response.lower() != 'y':
                print("å·²å–æ¶ˆ")
                sys.exit(0)

        # æ‰§è¡Œåˆ é™¤
        deleted_count = 0
        for file_info in files_to_delete:
            try:
                os.remove(file_info['path'])
                deleted_count += 1
            except Exception as e:
                logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_info['name']}: {e}")

        print(f"âœ… å·²åˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶")


if __name__ == "__main__":
    main()
